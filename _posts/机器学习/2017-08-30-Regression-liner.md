---
layout: post
title: 回归：线性回归
category: 机器学习
tags: SVM
keywords: Regression 机器学习
description: 
---

[TOC]

## 引言
    个人理解，用一条线做出来的功能，逻辑都是差不多的，因为直线的方程表达式就是ax_by+cz+.....wn = 0。
    分类问题：
    SVM 超平面(线)的上方下方问题
    Logistic Regression 线计算值，经过sigmod映射(sigmod可以理解为概率计算，几何上看)还是线上线下问题
    回归问题：
    XX Linear Regression 得到系数矩阵(a,b,c...w)后，求解(x,y,z...n)的值的问题。

## 线性回归思想
<pre><code>
    二维坐标系上有这么一条线(直线曲线都行)，给出个方程表达式y=f(x),给出点x可以得到y。
    预测就是知道了x求y。我们要是知道方程f(x)就好办了。
    > 1.  怎么确定这个f(x)呢。而且点x与对应值y怎么就一定满足f(x).
    > 所以我们的f(x)并不是x与y的映射关系，只是尽量的去拟合这些关系。
    
    > 2.  怎么的f(x)才拟合得好
    > 误差 ： 预测值与真实值的差值 f(x)-y
    > 因为差值有正负，多个差值累加会相互抵消(比如：点a -> f(a)=1,y=0.5 -> 差值0.5，点b -> f(b)=0.5,y=1 -> 差值-0.5,总误差为0，这是不合理的)
    
    > 3.平分误差: sum([y-f(x)]^2)
</code></pre>

### 目标函数
$$ 
min_{w}\sum_{i=1}^{n}(y_i - f(x_i) )^2
$$

